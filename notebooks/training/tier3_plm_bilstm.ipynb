{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73d1139",
   "metadata": {},
   "source": [
    "# Tier 3: PLM (ESM-2) Embeddings + BiLSTM Training\n",
    "\n",
    "This notebook implements training for the **Tier 3** architecture:\n",
    "- **Pre-computed ESM-2 embeddings** as input (1280-dim)\n",
    "- Optional 1D CNN for local refinement\n",
    "- BiLSTM for sequential modeling\n",
    "\n",
    "## Prerequisites\n",
    "Run the embedding extraction script first to generate ESM-2 embeddings:\n",
    "```bash\n",
    "python scripts/extract_embeddings.py\n",
    "```\n",
    "\n",
    "## Expected Performance\n",
    "- Q3 Accuracy: ~88-91%\n",
    "- Q8 Accuracy: ~77-82%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcec65",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import Tier3Config, LEAKAGE_TRAIN_IDS\n",
    "from src.data import PLMEmbeddingDataset, collate_fn\n",
    "from src.models.tier3_plm_bilstm import PLMBiLSTM\n",
    "from src.losses import get_multitask_loss\n",
    "from src.augmentation import EmbeddingAugmenter\n",
    "from src.metrics import evaluate_model, plot_confusion_matrix\n",
    "from src.training import Trainer, create_optimizer, create_scheduler, plot_training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93d78b",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Tier3Config(\n",
    "    # Data\n",
    "    max_seq_length=512,\n",
    "    batch_size=32,\n",
    "    \n",
    "    # Model\n",
    "    embedding_dim=1280,  # ESM-2 650M\n",
    "    embeddings_path='../../data/embeddings',\n",
    "    \n",
    "    use_cnn=True,\n",
    "    cnn_filters=128,\n",
    "    cnn_kernels=[3, 5],\n",
    "    \n",
    "    lstm_hidden=256,\n",
    "    lstm_layers=2,\n",
    "    lstm_dropout=0.2,\n",
    "    \n",
    "    fc_hidden=256,\n",
    "    fc_dropout=0.2,\n",
    "    \n",
    "    # Training\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    gradient_clip=1.0,\n",
    "    \n",
    "    # Loss\n",
    "    focal_gamma=2.0,\n",
    "    q8_loss_weight=1.0,\n",
    "    q3_loss_weight=0.5,\n",
    "    \n",
    "    # Augmentation\n",
    "    augmentation_level=2,  # Light noise injection for embeddings\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir='../../checkpoints/tier3_plm_bilstm',\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Embedding dim: {config.embedding_dim}\")\n",
    "print(f\"  Use CNN: {config.use_cnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96087ab",
   "metadata": {},
   "source": [
    "## 3. Check Embeddings Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = Path(config.embeddings_path)\n",
    "\n",
    "if not embeddings_dir.exists():\n",
    "    print(\"⚠️  Embeddings directory not found!\")\n",
    "    print(f\"   Expected at: {embeddings_dir.absolute()}\")\n",
    "    print(\"\\nTo extract embeddings, run:\")\n",
    "    print(\"   python scripts/extract_embeddings.py\")\n",
    "    print(\"\\nAlternatively, using on-the-fly embedding extraction (slower)...\")\n",
    "    USE_PRECOMPUTED = False\n",
    "else:\n",
    "    embedding_files = list(embeddings_dir.glob(\"*.pt\"))\n",
    "    print(f\"✅ Found {len(embedding_files)} pre-computed embeddings\")\n",
    "    USE_PRECOMPUTED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475facae",
   "metadata": {},
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1341704",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRECOMPUTED:\n",
    "    # Use PLM embedding dataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load full data for splitting\n",
    "    train_df = pd.read_csv('../../data/train.csv')\n",
    "    train_df = train_df[~train_df['id'].isin(LEAKAGE_TRAIN_IDS)].reset_index(drop=True)\n",
    "    \n",
    "    # Split\n",
    "    np.random.seed(SEED)\n",
    "    val_size = int(len(train_df) * 0.1)\n",
    "    val_indices = np.random.choice(len(train_df), val_size, replace=False)\n",
    "    train_indices = [i for i in range(len(train_df)) if i not in val_indices]\n",
    "    \n",
    "    train_split = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    val_split = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    train_split.to_csv('/tmp/plm_train.csv', index=False)\n",
    "    val_split.to_csv('/tmp/plm_val.csv', index=False)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PLMEmbeddingDataset(\n",
    "        '/tmp/plm_train.csv',\n",
    "        config.embeddings_path,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "    \n",
    "    val_dataset = PLMEmbeddingDataset(\n",
    "        '/tmp/plm_val.csv',\n",
    "        config.embeddings_path,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # On-the-fly embedding extraction using OnTheFlyPLMDataset\n",
    "    print(\"Using on-the-fly embedding extraction...\")\n",
    "    print(\"⚠️  This is slower. Pre-compute embeddings for faster training.\")\n",
    "    \n",
    "    from transformers import EsmTokenizer, EsmModel\n",
    "    from src.data import OnTheFlyPLMDataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load ESM-2 model\n",
    "    ESM_MODEL = \"facebook/esm2_t33_650M_UR50D\"\n",
    "    print(f\"Loading {ESM_MODEL}...\")\n",
    "    tokenizer = EsmTokenizer.from_pretrained(ESM_MODEL)\n",
    "    esm_model = EsmModel.from_pretrained(ESM_MODEL)\n",
    "    esm_model = esm_model.to(DEVICE)\n",
    "    esm_model.eval()\n",
    "    print(f\"✅ ESM-2 loaded\")\n",
    "    \n",
    "    # Load and split data\n",
    "    train_df = pd.read_csv('../../data/train.csv')\n",
    "    train_df = train_df[~train_df['id'].isin(LEAKAGE_TRAIN_IDS)].reset_index(drop=True)\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    val_size = int(len(train_df) * 0.1)\n",
    "    val_indices = np.random.choice(len(train_df), val_size, replace=False)\n",
    "    train_indices = [i for i in range(len(train_df)) if i not in val_indices]\n",
    "    \n",
    "    train_split = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    val_split = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    train_split.to_csv('/tmp/plm_train.csv', index=False)\n",
    "    val_split.to_csv('/tmp/plm_val.csv', index=False)\n",
    "    \n",
    "    # Create on-the-fly datasets\n",
    "    train_dataset = OnTheFlyPLMDataset(\n",
    "        '/tmp/plm_train.csv',\n",
    "        esm_model=esm_model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=DEVICE,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "    \n",
    "    val_dataset = OnTheFlyPLMDataset(\n",
    "        '/tmp/plm_val.csv',\n",
    "        esm_model=esm_model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=DEVICE,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "    \n",
    "    # Use smaller batch size due to on-the-fly extraction\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=max(1, config.batch_size // 4),  # Smaller batch\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,  # No multiprocessing with GPU model\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=max(1, config.batch_size // 4),\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952536c1",
   "metadata": {},
   "source": [
    "## 5. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PLMBiLSTM(\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    use_cnn=config.use_cnn,\n",
    "    cnn_filters=config.cnn_filters,\n",
    "    cnn_kernels=config.cnn_kernels,\n",
    "    lstm_hidden=config.lstm_hidden,\n",
    "    lstm_layers=config.lstm_layers,\n",
    "    lstm_dropout=config.lstm_dropout,\n",
    "    fc_hidden=config.fc_hidden,\n",
    "    fc_dropout=config.fc_dropout,\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38096e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "sample_batch = next(iter(train_loader))\n",
    "test_input = sample_batch['features'].to(DEVICE)\n",
    "q8_out, q3_out = model(test_input)\n",
    "print(f\"Q8 output shape: {q8_out.shape}\")\n",
    "print(f\"Q3 output shape: {q3_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0dc8a8",
   "metadata": {},
   "source": [
    "## 6. Loss Function & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bc497",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = get_multitask_loss(\n",
    "    loss_type='focal',\n",
    "    q8_weight=config.q8_loss_weight,\n",
    "    q3_weight=config.q3_loss_weight,\n",
    "    gamma=config.focal_gamma,\n",
    ")\n",
    "\n",
    "optimizer = create_optimizer(\n",
    "    model,\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = create_scheduler(\n",
    "    optimizer,\n",
    "    scheduler_type='cosine',\n",
    "    num_epochs=config.max_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7268a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=config.checkpoint_dir,\n",
    "    gradient_clip=config.gradient_clip,\n",
    "    use_amp=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train(\n",
    "    num_epochs=config.max_epochs,\n",
    "    patience=config.patience,\n",
    "    save_every=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84ab4c",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_history(history, save_path=f'{config.checkpoint_dir}/training_history.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'{config.checkpoint_dir}/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "report = evaluate_model(\n",
    "    model=model,\n",
    "    dataloader=val_loader,\n",
    "    device=DEVICE,\n",
    "    compute_sov=True,\n",
    ")\n",
    "\n",
    "report.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271725b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import SST8_CLASSES, SST3_CLASSES\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    report.q8_confusion_matrix,\n",
    "    SST8_CLASSES,\n",
    "    title='Q8 Confusion Matrix (Tier 3 - PLM)',\n",
    "    save_path=f'{config.checkpoint_dir}/q8_confusion_matrix.png',\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    report.q3_confusion_matrix,\n",
    "    SST3_CLASSES,\n",
    "    title='Q3 Confusion Matrix (Tier 3 - PLM)',\n",
    "    save_path=f'{config.checkpoint_dir}/q3_confusion_matrix.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2fdd9",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TIER 3 (PLM + BiLSTM) TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Results:\")\n",
    "print(f\"  Q8 Accuracy: {report.q8_accuracy:.4f} ({report.q8_accuracy*100:.2f}%)\")\n",
    "print(f\"  Q3 Accuracy: {report.q3_accuracy:.4f} ({report.q3_accuracy*100:.2f}%)\")\n",
    "print(f\"  Q8 Macro F1: {report.q8_macro_f1:.4f}\")\n",
    "print(f\"  Q3 Macro F1: {report.q3_macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nThe power of PLM embeddings:\")\n",
    "print(f\"  - Pre-trained on 250M+ protein sequences\")\n",
    "print(f\"  - Captures evolutionary and structural information\")\n",
    "print(f\"  - No need for MSA (faster inference)\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
