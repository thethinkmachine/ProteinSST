{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171e92c2",
   "metadata": {},
   "source": [
    "# Tier 5: Fine-tuned ESM-2 Training\n",
    "\n",
    "This notebook implements training for the **Tier 5** architecture:\n",
    "- **Fine-tuned ESM-2** pre-trained language model\n",
    "- Task-specific output heads for Q8 and Q3\n",
    "- Gradient checkpointing for memory efficiency\n",
    "- Layer-wise learning rate decay\n",
    "\n",
    "## Requirements\n",
    "- GPU with at least 16GB VRAM (24GB+ recommended)\n",
    "- `transformers` library\n",
    "\n",
    "## Expected Performance\n",
    "- Q3 Accuracy: ~91-93%\n",
    "- Q8 Accuracy: ~80-85%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc4f64",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c945e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "if DEVICE == 'cpu':\n",
    "    print(\"⚠️  Warning: Fine-tuning ESM-2 on CPU will be very slow!\")\n",
    "    print(\"   Consider using Tier 3 or 4 instead for CPU training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if gpu_memory < 12:\n",
    "        print(\"⚠️  Limited GPU memory. Using smaller ESM-2 model.\")\n",
    "        ESM_MODEL = \"facebook/esm2_t12_35M_UR50D\"  # 35M params\n",
    "    else:\n",
    "        ESM_MODEL = \"facebook/esm2_t33_650M_UR50D\"  # 650M params\n",
    "else:\n",
    "    ESM_MODEL = \"facebook/esm2_t6_8M_UR50D\"  # Smallest for CPU\n",
    "\n",
    "print(f\"Using ESM-2 model: {ESM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import Tier5Config, LEAKAGE_TRAIN_IDS\n",
    "from src.models.tier5_esm2_finetune import ESM2FineTune, ESM2Dataset, esm2_collate_fn\n",
    "from src.losses import get_multitask_loss\n",
    "from src.metrics import evaluate_model, plot_confusion_matrix\n",
    "from src.training import Trainer, plot_training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63ce95",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Tier5Config(\n",
    "    # Data\n",
    "    max_seq_length=512,\n",
    "    batch_size=8,  # Small for memory\n",
    "    \n",
    "    # Model\n",
    "    esm_model=ESM_MODEL,\n",
    "    freeze_layers=0,  # Full fine-tune (set >0 to freeze layers)\n",
    "    \n",
    "    fc_hidden=512,\n",
    "    fc_dropout=0.1,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Training\n",
    "    learning_rate=1e-5,  # Low LR for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    max_epochs=30,  # Fewer epochs for fine-tuning\n",
    "    patience=7,\n",
    "    gradient_clip=1.0,\n",
    "    \n",
    "    # Loss\n",
    "    focal_gamma=2.0,\n",
    "    q8_loss_weight=1.0,\n",
    "    q3_loss_weight=0.5,\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir='../../checkpoints/tier5_esm2_finetune',\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.esm_model}\")\n",
    "print(f\"  Frozen layers: {config.freeze_layers}\")\n",
    "print(f\"  Gradient checkpointing: {config.gradient_checkpointing}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd429c1",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = EsmTokenizer.from_pretrained(ESM_MODEL)\n",
    "\n",
    "# Load and split data\n",
    "train_df = pd.read_csv('../../data/train.csv')\n",
    "train_df = train_df[~train_df['id'].isin(LEAKAGE_TRAIN_IDS)].reset_index(drop=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "val_size = int(len(train_df) * 0.1)\n",
    "val_indices = np.random.choice(len(train_df), val_size, replace=False)\n",
    "train_indices = [i for i in range(len(train_df)) if i not in val_indices]\n",
    "\n",
    "train_split = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "val_split = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "\n",
    "train_split.to_csv('/tmp/esm2_train.csv', index=False)\n",
    "val_split.to_csv('/tmp/esm2_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ESM2Dataset(\n",
    "    '/tmp/esm2_train.csv',\n",
    "    tokenizer,\n",
    "    max_length=config.max_seq_length,\n",
    ")\n",
    "\n",
    "val_dataset = ESM2Dataset(\n",
    "    '/tmp/esm2_val.csv',\n",
    "    tokenizer,\n",
    "    max_length=config.max_seq_length,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=esm2_collate_fn,\n",
    "    num_workers=2,  # Fewer workers for tokenization\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=esm2_collate_fn,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22819573",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESM2FineTune(\n",
    "    model_name=ESM_MODEL,\n",
    "    freeze_layers=config.freeze_layers,\n",
    "    fc_hidden=config.fc_hidden,\n",
    "    fc_dropout=config.fc_dropout,\n",
    "    gradient_checkpointing=config.gradient_checkpointing,\n",
    ")\n",
    "\n",
    "total_params = model.count_parameters(trainable_only=False)\n",
    "trainable_params = model.count_parameters(trainable_only=True)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "sample_batch = next(iter(train_loader))\n",
    "input_ids = sample_batch['input_ids'].to(DEVICE)\n",
    "attention_mask = sample_batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    q8_out, q3_out = model(input_ids, attention_mask)\n",
    "\n",
    "print(f\"Q8 output shape: {q8_out.shape}\")\n",
    "print(f\"Q3 output shape: {q3_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b2e78",
   "metadata": {},
   "source": [
    "## 5. Loss & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = get_multitask_loss(\n",
    "    loss_type='focal',\n",
    "    q8_weight=config.q8_loss_weight,\n",
    "    q3_weight=config.q3_loss_weight,\n",
    "    gamma=config.focal_gamma,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de38052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer-wise learning rate decay\n",
    "param_groups = model.get_layer_lrs(\n",
    "    base_lr=config.learning_rate,\n",
    "    lr_decay=0.95,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=config.weight_decay)\n",
    "\n",
    "# Print LR by layer\n",
    "print(\"Layer-wise learning rates:\")\n",
    "for pg in param_groups[:5]:  # First 5\n",
    "    print(f\"  {pg['name']}: {pg['lr']:.2e}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd304548",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=config.max_epochs // 3,\n",
    "    T_mult=2,\n",
    "    eta_min=1e-7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d3301",
   "metadata": {},
   "source": [
    "## 6. Custom Training Loop for ESM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a2069",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ESM-2 requires special handling for forward pass\n",
    "class ESM2Trainer(Trainer):\n",
    "    \"\"\"Modified trainer for ESM-2 fine-tuning.\"\"\"\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {self.current_epoch}')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            q8_targets = batch['sst8'].to(self.device)\n",
    "            q3_targets = batch['sst3'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.use_amp:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    q8_logits, q3_logits = self.model(input_ids, attention_mask)\n",
    "                    loss, q8_loss, q3_loss = self.loss_fn(\n",
    "                        q8_logits, q8_targets, q3_logits, q3_targets\n",
    "                    )\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                q8_logits, q3_logits = self.model(input_ids, attention_mask)\n",
    "                loss, q8_loss, q3_loss = self.loss_fn(\n",
    "                    q8_logits, q8_targets, q3_logits, q3_targets\n",
    "                )\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            batch_size = input_ids.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        return {'loss': total_loss / total_samples, 'q8_loss': 0, 'q3_loss': 0}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_q8_preds = []\n",
    "        all_q8_targets = []\n",
    "        all_q3_preds = []\n",
    "        all_q3_targets = []\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch in self.val_loader:\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            q8_targets = batch['sst8'].to(self.device)\n",
    "            q3_targets = batch['sst3'].to(self.device)\n",
    "            \n",
    "            q8_logits, q3_logits = self.model(input_ids, attention_mask)\n",
    "            loss, _, _ = self.loss_fn(q8_logits, q8_targets, q3_logits, q3_targets)\n",
    "            \n",
    "            batch_size = input_ids.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            all_q8_preds.append(q8_logits)\n",
    "            all_q8_targets.append(q8_targets)\n",
    "            all_q3_preds.append(q3_logits)\n",
    "            all_q3_targets.append(q3_targets)\n",
    "        \n",
    "        from src.metrics import compute_q8_accuracy, compute_q3_accuracy\n",
    "        \n",
    "        all_q8_preds = torch.cat(all_q8_preds, dim=0)\n",
    "        all_q8_targets = torch.cat(all_q8_targets, dim=0)\n",
    "        all_q3_preds = torch.cat(all_q3_preds, dim=0)\n",
    "        all_q3_targets = torch.cat(all_q3_targets, dim=0)\n",
    "        \n",
    "        q8_accuracy = compute_q8_accuracy(all_q8_preds, all_q8_targets)\n",
    "        q3_accuracy = compute_q3_accuracy(all_q3_preds, all_q3_targets)\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / total_samples,\n",
    "            'q8_loss': 0,\n",
    "            'q3_loss': 0,\n",
    "            'q8_accuracy': q8_accuracy,\n",
    "            'q3_accuracy': q3_accuracy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb68718",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ESM2Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=config.checkpoint_dir,\n",
    "    gradient_clip=config.gradient_clip,\n",
    "    use_amp=torch.cuda.is_available(),\n",
    "    use_tracking=True,\n",
    "    experiment_name='tier5_finetuned_esm2',\n",
    "    hub_model_id='thethinkmachine/ProteinSST-ESM2',\n",
    "    training_config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train(\n",
    "    num_epochs=config.max_epochs,\n",
    "    patience=config.patience,\n",
    "    save_every=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bfdd41",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330802ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_history(history, save_path=f'{config.checkpoint_dir}/training_history.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "checkpoint = torch.load(f'{config.checkpoint_dir}/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Custom evaluation for ESM-2\n",
    "model.eval()\n",
    "\n",
    "all_q8_preds = []\n",
    "all_q8_targets = []\n",
    "all_q3_preds = []\n",
    "all_q3_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        q8_targets = batch['sst8'].to(DEVICE)\n",
    "        q3_targets = batch['sst3'].to(DEVICE)\n",
    "        \n",
    "        q8_logits, q3_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        all_q8_preds.append(q8_logits)\n",
    "        all_q8_targets.append(q8_targets)\n",
    "        all_q3_preds.append(q3_logits)\n",
    "        all_q3_targets.append(q3_targets)\n",
    "\n",
    "all_q8_preds = torch.cat(all_q8_preds, dim=0)\n",
    "all_q8_targets = torch.cat(all_q8_targets, dim=0)\n",
    "all_q3_preds = torch.cat(all_q3_preds, dim=0)\n",
    "all_q3_targets = torch.cat(all_q3_targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import (\n",
    "    compute_q8_accuracy, compute_q3_accuracy,\n",
    "    compute_sst8_per_class_metrics, compute_sst3_per_class_metrics,\n",
    "    compute_confusion_matrix,\n",
    ")\n",
    "from src.config import SST8_CLASSES, SST3_CLASSES\n",
    "\n",
    "q8_accuracy = compute_q8_accuracy(all_q8_preds, all_q8_targets)\n",
    "q3_accuracy = compute_q3_accuracy(all_q3_preds, all_q3_targets)\n",
    "\n",
    "q8_per_class = compute_sst8_per_class_metrics(all_q8_preds, all_q8_targets)\n",
    "q3_per_class = compute_sst3_per_class_metrics(all_q3_preds, all_q3_targets)\n",
    "\n",
    "q8_cm = compute_confusion_matrix(all_q8_preds, all_q8_targets, 8)\n",
    "q3_cm = compute_confusion_matrix(all_q3_preds, all_q3_targets, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7041103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TIER 5 (Fine-tuned ESM-2) EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQ8 Accuracy: {q8_accuracy:.4f} ({q8_accuracy*100:.2f}%)\")\n",
    "print(f\"Q3 Accuracy: {q3_accuracy:.4f} ({q3_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nQ8 Macro F1: {q8_per_class['macro_avg']['f1']:.4f}\")\n",
    "print(f\"Q3 Macro F1: {q3_per_class['macro_avg']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    q8_cm,\n",
    "    SST8_CLASSES,\n",
    "    title='Q8 Confusion Matrix (Tier 5 - ESM-2)',\n",
    "    save_path=f'{config.checkpoint_dir}/q8_confusion_matrix.png',\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    q3_cm,\n",
    "    SST3_CLASSES,\n",
    "    title='Q3 Confusion Matrix (Tier 5 - ESM-2)',\n",
    "    save_path=f'{config.checkpoint_dir}/q3_confusion_matrix.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286e53f",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd786e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TIER 5 (Fine-tuned ESM-2) TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Q8 Accuracy: {q8_accuracy:.4f} ({q8_accuracy*100:.2f}%)\")\n",
    "print(f\"  Q3 Accuracy: {q3_accuracy:.4f} ({q3_accuracy*100:.2f}%)\")\n",
    "print(f\"  Q8 Macro F1: {q8_per_class['macro_avg']['f1']:.4f}\")\n",
    "print(f\"  Q3 Macro F1: {q3_per_class['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nFine-tuning ESM-2 provides:\")\n",
    "print(f\"  - State-of-the-art performance\")\n",
    "print(f\"  - Transfer learning from 250M+ protein sequences\")\n",
    "print(f\"  - End-to-end differentiable pipeline\")\n",
    "print(f\"\\nNote: This is the most computationally expensive tier\")\n",
    "print(f\"      but delivers the best accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
