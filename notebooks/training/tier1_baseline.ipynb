{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86eaf9e",
   "metadata": {},
   "source": [
    "# ğŸ§¬ Tier 1: Baseline PLM Model Training\n",
    "\n",
    "This notebook implements the **Tier 1 Baseline** architecture for protein secondary structure prediction.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         TIER 1: BASELINE                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚   PLM Embeddings (L, D_plm)                                             â”‚\n",
    "â”‚          â”‚                                                              â”‚\n",
    "â”‚          â–¼                                                              â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚\n",
    "â”‚   â”‚  Linear(D_plm,  â”‚                                                   â”‚\n",
    "â”‚   â”‚     512)        â”‚  Feature Projection                               â”‚\n",
    "â”‚   â”‚   + GELU        â”‚                                                   â”‚\n",
    "â”‚   â”‚   + LayerNorm   â”‚                                                   â”‚\n",
    "â”‚   â”‚   + Dropout     â”‚                                                   â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚\n",
    "â”‚            â”‚                                                            â”‚\n",
    "â”‚            â–¼                                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚\n",
    "â”‚   â”‚  MTL Head       â”‚  q3discarding OR q3guided                         â”‚\n",
    "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                                                   â”‚\n",
    "â”‚   â”‚  â”‚ Q8 Head   â”‚â”€â”€â”¼â”€â”€â–¶ Q8 Logits (L, 8)                               â”‚\n",
    "â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                                   â”‚\n",
    "â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                                                   â”‚\n",
    "â”‚   â”‚  â”‚ Q3 Head   â”‚â”€â”€â”¼â”€â”€â–¶ Q3 Logits (L, 3)                               â”‚\n",
    "â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                                   â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **PLM Support** | Ankh, ESM-2, ProtBert |\n",
    "| **Parameters** | ~660K (lightweight) |\n",
    "| **Training Time** | ~5min/epoch on GPU |\n",
    "| **Use Case** | Baseline establishment, fast experimentation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7f424",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ğŸ–¥ï¸  Device: {DEVICE}\")\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library modules\n",
    "from src.config import (\n",
    "    Tier1Config, LEAKAGE_TRAIN_IDS,\n",
    "    SST8_WEIGHTS, SST3_WEIGHTS,\n",
    "    get_embedding_dim, PLM_EMBEDDING_DIMS,\n",
    ")\n",
    "from src.data import HDF5EmbeddingDataset, collate_fn\n",
    "from src.models import Tier1Baseline\n",
    "from src.losses import get_multitask_loss\n",
    "from src.training import Trainer, create_optimizer, create_scheduler, plot_training_history\n",
    "\n",
    "print(\"âœ“ Library modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fefeb2",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Choose your PLM and configure training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available PLMs\n",
    "print(\"Available PLMs:\")\n",
    "print(\"â”€\" * 50)\n",
    "for name, dim in PLM_EMBEDDING_DIMS.items():\n",
    "    print(f\"  {name:15} â”‚ dim={dim:4}\")\n",
    "print(\"â”€\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502122ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION - Modify these values\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "config = Tier1Config(\n",
    "    # PLM Selection\n",
    "    plm_name='ankh_base',\n",
    "    embeddings_path='../../data/embeddings/ankh_base.h5',\n",
    "    \n",
    "    # Model Architecture\n",
    "    fc_hidden=512,\n",
    "    fc_dropout=0.1,\n",
    "    \n",
    "    # MTL Head Strategy: 'q3discarding' or 'q3guided'\n",
    "    head_strategy='q3discarding',\n",
    "    head_hidden=256,\n",
    "    head_dropout=0.1,\n",
    "    \n",
    "    # Training\n",
    "    max_seq_length=512,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    gradient_clip=1.0,\n",
    "    \n",
    "    # Loss\n",
    "    focal_gamma=2.0,\n",
    "    q8_loss_weight=1.0,\n",
    "    q3_loss_weight=0.5,\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir='../../checkpoints/tier1_baseline',\n",
    "    \n",
    "    # Optional: Experiment Tracking & Hub\n",
    "    use_tracking=False,  # Set True to enable Trackio\n",
    "    experiment_name='tier1_baseline',\n",
    "    # hub_model_id='your-username/ProteinSST-Tier1',  # Uncomment to push to Hub\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print configuration summary\n",
    "print(\"\\n\" + \"â•\" * 60)\n",
    "print(\"TIER 1 BASELINE CONFIGURATION\")\n",
    "print(\"â•\" * 60)\n",
    "print(f\"\\nğŸ“¦ PLM: {config.plm_name}\")\n",
    "print(f\"   Embedding Dim: {get_embedding_dim(config.plm_name)}\")\n",
    "print(f\"   Embeddings: {config.embeddings_path}\")\n",
    "print(f\"\\nğŸ—ï¸  Architecture:\")\n",
    "print(f\"   FC Hidden: {config.fc_hidden}\")\n",
    "print(f\"   Head Strategy: {config.head_strategy}\")\n",
    "print(f\"\\nâš™ï¸  Training:\")\n",
    "print(f\"   Batch Size: {config.batch_size}\")\n",
    "print(f\"   Learning Rate: {config.learning_rate}\")\n",
    "print(f\"   Max Epochs: {config.max_epochs}\")\n",
    "print(f\"   Early Stopping Patience: {config.patience}\")\n",
    "print(\"â•\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400b070",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load pre-computed PLM embeddings from HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abe193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check embeddings exist\n",
    "embeddings_path = Path(config.embeddings_path)\n",
    "\n",
    "if not embeddings_path.exists():\n",
    "    print(f\"âŒ Embeddings not found: {embeddings_path}\")\n",
    "    print(f\"\\n   Run extraction first:\")\n",
    "    print(f\"   python scripts/extract_embeddings.py --plm {config.plm_name}\")\n",
    "else:\n",
    "    import h5py\n",
    "    with h5py.File(embeddings_path, 'r') as f:\n",
    "        train_count = len(f['train']) if 'train' in f else 0\n",
    "        cb513_count = len(f['cb513']) if 'cb513' in f else 0\n",
    "        plm_name = f.attrs.get('plm_name', 'unknown')\n",
    "        embedding_dim = f.attrs.get('embedding_dim', 0)\n",
    "    \n",
    "    print(f\"âœ“ Embeddings found: {embeddings_path}\")\n",
    "    print(f\"   PLM: {plm_name}\")\n",
    "    print(f\"   Embedding Dim: {embedding_dim}\")\n",
    "    print(f\"   Train samples: {train_count}\")\n",
    "    print(f\"   CB513 samples: {cb513_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "full_dataset = HDF5EmbeddingDataset(\n",
    "    csv_path='../../data/train.csv',\n",
    "    h5_path=config.embeddings_path,\n",
    "    dataset_name='train',\n",
    "    max_length=config.max_seq_length,\n",
    "    exclude_ids=LEAKAGE_TRAIN_IDS,\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "val_split = 0.1\n",
    "val_size = int(len(full_dataset) * val_split)\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Split:\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Val:   {len(val_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d27bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“¦ DataLoaders:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches:   {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23869e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"\\nğŸ” Sample Batch:\")\n",
    "print(f\"   Features shape: {sample_batch['features'].shape}\")\n",
    "print(f\"   SST8 shape:     {sample_batch['sst8'].shape}\")\n",
    "print(f\"   SST3 shape:     {sample_batch['sst3'].shape}\")\n",
    "print(f\"   Lengths:        {sample_batch['lengths'][:5].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b48077",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaccf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding dimension from PLM\n",
    "embedding_dim = get_embedding_dim(config.plm_name)\n",
    "\n",
    "# Create model\n",
    "model = Tier1Baseline(\n",
    "    embedding_dim=embedding_dim,\n",
    "    fc_hidden=config.fc_hidden,\n",
    "    fc_dropout=config.fc_dropout,\n",
    "    head_strategy=config.head_strategy,\n",
    "    head_hidden=config.head_hidden,\n",
    "    head_dropout=config.head_dropout,\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ—ï¸  Model Created:\")\n",
    "print(f\"   Type: Tier1Baseline\")\n",
    "print(f\"   Embedding Dim: {embedding_dim}\")\n",
    "print(f\"   FC Hidden: {config.fc_hidden}\")\n",
    "print(f\"   Head Strategy: {config.head_strategy}\")\n",
    "print(f\"\\nğŸ“ˆ Total Parameters: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = sample_batch['features'].to(DEVICE)\n",
    "    q8_out, q3_out = model(test_input)\n",
    "\n",
    "print(\"\\nâœ“ Forward Pass Test:\")\n",
    "print(f\"   Input:  {test_input.shape}\")\n",
    "print(f\"   Q8 Out: {q8_out.shape}\")\n",
    "print(f\"   Q3 Out: {q3_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e5a271",
   "metadata": {},
   "source": [
    "## 5. Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task loss with focal loss and class weights\n",
    "loss_fn = get_multitask_loss(\n",
    "    loss_type='focal',\n",
    "    q8_weight=config.q8_loss_weight,\n",
    "    q3_weight=config.q3_loss_weight,\n",
    "    q8_class_weights=SST8_WEIGHTS.to(DEVICE),\n",
    "    q3_class_weights=SST3_WEIGHTS.to(DEVICE),\n",
    "    gamma=config.focal_gamma,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“‰ Loss Function: MultiTaskLoss (Focal)\")\n",
    "print(f\"   Q8 Weight: {config.q8_loss_weight}\")\n",
    "print(f\"   Q3 Weight: {config.q3_loss_weight}\")\n",
    "print(f\"   Focal Gamma: {config.focal_gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211cb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = create_optimizer(\n",
    "    model,\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    optimizer_type='adamw',\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = create_scheduler(\n",
    "    optimizer,\n",
    "    scheduler_type='cosine',\n",
    "    num_epochs=config.max_epochs,\n",
    ")\n",
    "\n",
    "print(\"\\nâš¡ Optimizer: AdamW\")\n",
    "print(f\"   Learning Rate: {config.learning_rate}\")\n",
    "print(f\"   Weight Decay: {config.weight_decay}\")\n",
    "print(\"\\nğŸ“… Scheduler: CosineAnnealingWarmRestarts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f97e3",
   "metadata": {},
   "source": [
    "## 6. Training with Library Trainer\n",
    "\n",
    "Using the `Trainer` class from `src/training.py` which includes:\n",
    "- Early stopping based on harmonic mean of Q8 and Q3 F1 scores\n",
    "- Automatic checkpointing\n",
    "- Optional Trackio experiment tracking\n",
    "- Optional HuggingFace Hub push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer using library\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=config.checkpoint_dir,\n",
    "    gradient_clip=config.gradient_clip,\n",
    "    log_every=100,\n",
    "    use_amp=torch.cuda.is_available(),  # Use FP16 on GPU\n",
    "    use_tracking=config.use_tracking,\n",
    "    experiment_name=config.experiment_name,\n",
    "    hub_model_id=config.hub_model_id if hasattr(config, 'hub_model_id') else None,\n",
    "    training_config=config.__dict__,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(f\"   Checkpoint dir: {config.checkpoint_dir}\")\n",
    "print(f\"   Mixed Precision: {trainer.use_amp}\")\n",
    "print(f\"   Tracking: {trainer.use_tracking}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "history = trainer.train(\n",
    "    num_epochs=config.max_epochs,\n",
    "    patience=config.patience,\n",
    "    save_every=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d89895",
   "metadata": {},
   "source": [
    "## 7. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b321fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history using library function\n",
    "fig = plot_training_history(\n",
    "    history,\n",
    "    save_path=str(Path(config.checkpoint_dir) / 'training_curves.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e0dc7",
   "metadata": {},
   "source": [
    "## 8. Evaluation on CB513 Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CB513 test set if available\n",
    "cb513_h5_path = Path(config.embeddings_path)\n",
    "\n",
    "if cb513_h5_path.exists():\n",
    "    try:\n",
    "        cb513_dataset = HDF5EmbeddingDataset(\n",
    "            csv_path='../../data/cb513.csv',\n",
    "            h5_path=config.embeddings_path,\n",
    "            dataset_name='cb513',\n",
    "            max_length=config.max_seq_length,\n",
    "        )\n",
    "        \n",
    "        cb513_loader = DataLoader(\n",
    "            cb513_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ CB513 test set loaded: {len(cb513_dataset)} samples\")\n",
    "        \n",
    "        # Load best model and evaluate\n",
    "        best_checkpoint = torch.load(\n",
    "            Path(config.checkpoint_dir) / 'best_model.pt',\n",
    "            map_location=DEVICE\n",
    "        )\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Temporarily swap val_loader to evaluate on CB513\n",
    "        original_val_loader = trainer.val_loader\n",
    "        trainer.val_loader = cb513_loader\n",
    "        \n",
    "        test_metrics = trainer.validate()\n",
    "        \n",
    "        trainer.val_loader = original_val_loader  # Restore\n",
    "        \n",
    "        print(\"\\n\" + \"â•\" * 60)\n",
    "        print(\"ğŸ“Š CB513 TEST SET RESULTS\")\n",
    "        print(\"â•\" * 60)\n",
    "        print(f\"   Q8 Accuracy: {test_metrics['q8_accuracy']:.4f}\")\n",
    "        print(f\"   Q3 Accuracy: {test_metrics['q3_accuracy']:.4f}\")\n",
    "        print(f\"   Q8 F1:       {test_metrics['q8_f1']:.4f}\")\n",
    "        print(f\"   Q3 F1:       {test_metrics['q3_f1']:.4f}\")\n",
    "        print(\"â•\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not evaluate on CB513: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ CB513 embeddings not found. Run extraction first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9d96c",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"â•\" * 60)\n",
    "print(\"ğŸ‰ TRAINING COMPLETE\")\n",
    "print(\"â•\" * 60)\n",
    "print(f\"\\nğŸ“ˆ Best Results:\")\n",
    "print(f\"   Harmonic F1: {trainer.best_harmonic_f1:.4f}\")\n",
    "print(f\"   Q8 F1:       {trainer.best_q8_f1:.4f}\")\n",
    "print(f\"   Q8 Accuracy: {trainer.best_q8_accuracy:.4f}\")\n",
    "print(f\"\\nğŸ’¾ Checkpoints saved to: {config.checkpoint_dir}\")\n",
    "print(\"â•\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
