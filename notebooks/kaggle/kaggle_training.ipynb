{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6616b5b",
   "metadata": {},
   "source": [
    "# ğŸ§¬ ProteinSST - Kaggle Training Notebook\n",
    "\n",
    "This notebook is designed for the **Kaggle Protein Secondary Structure Prediction** competition.\n",
    "\n",
    "## Quick Start\n",
    "1. Upload `src/` folder as a Kaggle Dataset (or clone from GitHub)\n",
    "2. Configure the TIER, PLM, and training mode\n",
    "3. Run all cells to train and generate `submission.csv`\n",
    "\n",
    "## Training Modes\n",
    "\n",
    "| Mode | `FROZEN_PLM` | Description | GPU Memory |\n",
    "|------|-------------|-------------|------------|\n",
    "| **Frozen** | `True` | Uses pre-extracted embeddings (fast) | ~4GB |\n",
    "| **FFT** | `False` | Trains PLM end-to-end (best accuracy) | ~16GB+ |\n",
    "\n",
    "## Architecture Tiers\n",
    "\n",
    "| Tier | Architecture | Parameters | Best For |\n",
    "|------|-------------|------------|----------|\n",
    "| 1 | PLM â†’ FC â†’ Head | ~500K | Fast baseline |\n",
    "| 2 | PLM â†’ CNN â†’ Head | ~800K | Local patterns |\n",
    "| 3 | PLM â†’ CNN â†’ RNN â†’ Head | ~2M | Sequential dependencies |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b601511",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# KAGGLE SETUP - Run this cell first!\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Option 1: If you uploaded src/ as a dataset named 'proteinsst-src'\n",
    "# import sys\n",
    "# sys.path.insert(0, '/kaggle/input/proteinsst-src')\n",
    "\n",
    "# Option 2: Clone from GitHub (uncomment if needed)\n",
    "!git clone https://github.com/thethinkmachine/ProteinSST.git\n",
    "# import sys\n",
    "# sys.path.insert(0, '/kaggle/working/ProteinSST')\n",
    "\n",
    "# For local testing, use this:\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# Install dependencies (if not already installed)\n",
    "!pip install -q h5py transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7275719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f5fba9",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**âš ï¸ IMPORTANT: Configure these settings before running!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9376073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¯ MAIN CONFIGURATION - CHANGE THESE!\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Model Selection\n",
    "TIER = 1  # Options: 1 (baseline), 2 (CNN), 3 (CNN+RNN)\n",
    "PLM_NAME = 'protbert'  # Options: 'protbert', 'esm2_8m', 'esm2_35m', 'esm2_650m'\n",
    "\n",
    "# Training Mode\n",
    "FROZEN_PLM = True  # True: Use pre-extracted embeddings (fast, ~4GB GPU)\n",
    "                   # False: Train PLM end-to-end (FFT, best accuracy, ~16GB+ GPU)\n",
    "\n",
    "# Architecture Options (Tier 2 & 3 only)\n",
    "CNN_TYPE = 'multiscale'  # Options: 'multiscale', 'deep'\n",
    "RNN_TYPE = 'lstm'  # Options: 'lstm', 'gru', 'rnn' (Tier 3 only)\n",
    "\n",
    "# Training Settings\n",
    "LOSS_TYPE = 'focal'  # Options: 'focal', 'crf', 'weighted_ce', 'ce'\n",
    "HEAD_STRATEGY = 'q3guided'  # Options: 'q3guided', 'q3discarding'\n",
    "\n",
    "# Adjust batch size and learning rate based on mode\n",
    "if FROZEN_PLM:\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 1e-4\n",
    "    MAX_EPOCHS = 50\n",
    "    PATIENCE = 10\n",
    "else:\n",
    "    # FFT mode needs smaller batch and lower LR\n",
    "    BATCH_SIZE = 4 if TIER == 3 else 8\n",
    "    LEARNING_RATE = 5e-6 if TIER == 3 else 1e-5\n",
    "    MAX_EPOCHS = 15\n",
    "    PATIENCE = 5\n",
    "\n",
    "# Output\n",
    "GENERATE_SUBMISSION = True\n",
    "SEED = 42\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“ PATHS - Adjust for your Kaggle datasets\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# For Kaggle:\n",
    "# TRAIN_CSV = '/kaggle/input/your-competition-name/train.csv'\n",
    "# TEST_CSV = '/kaggle/input/your-competition-name/test.csv'\n",
    "# EMBEDDINGS_PATH = '/kaggle/input/proteinsst-embeddings/protbert.h5'  # Only needed if FROZEN_PLM=True\n",
    "# OUTPUT_DIR = '/kaggle/working'\n",
    "\n",
    "# For local testing:\n",
    "TRAIN_CSV = '../../data/train.csv'\n",
    "TEST_CSV = '../../data/test.csv'\n",
    "EMBEDDINGS_PATH = f'../../data/embeddings/{PLM_NAME}.h5'  # Only used if FROZEN_PLM=True\n",
    "OUTPUT_DIR = '../../checkpoints/kaggle'\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'â•' * 60}\")\n",
    "print(f\"CONFIGURATION SUMMARY\")\n",
    "print(f\"{'â•' * 60}\")\n",
    "print(f\"ğŸ”§ Mode: {'Frozen PLM' if FROZEN_PLM else 'ğŸ”¥ Full Fine-Tuning (FFT)'}\")\n",
    "print(f\"ğŸ—ï¸  Tier: {TIER}\")\n",
    "print(f\"ğŸ“¦ PLM: {PLM_NAME}\")\n",
    "if TIER >= 2:\n",
    "    print(f\"ğŸ”² CNN: {CNN_TYPE}\")\n",
    "if TIER >= 3:\n",
    "    print(f\"ğŸ”„ RNN: {RNN_TYPE}\")\n",
    "print(f\"ğŸ“‰ Loss: {LOSS_TYPE}\")\n",
    "print(f\"ğŸ¯ Head: {HEAD_STRATEGY}\")\n",
    "print(f\"ğŸ“Š Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"ğŸ“ˆ Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"ğŸ–¥ï¸  Device: {DEVICE}\")\n",
    "print(f\"{'â•' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295593a",
   "metadata": {},
   "source": [
    "## 3. Import ProteinSST Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import (\n",
    "    Tier1Config, Tier2Config, Tier3Config,\n",
    "    LEAKAGE_TRAIN_IDS, get_embedding_dim, IDX_TO_SST8,\n",
    ")\n",
    "from src.data import HDF5EmbeddingDataset, collate_fn\n",
    "from src.models import Tier1Baseline, Tier2CNN, Tier3CNNRNN, SequenceDataset, collate_fn_sequences\n",
    "from src.losses import get_multitask_loss\n",
    "from src.training import Trainer, create_optimizer, create_scheduler\n",
    "\n",
    "print(\"âœ“ ProteinSST modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e05078",
   "metadata": {},
   "source": [
    "## 4. Create Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39673ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build config based on selected tier\n",
    "embedding_dim = get_embedding_dim(PLM_NAME)\n",
    "\n",
    "if TIER == 1:\n",
    "    config = Tier1Config(\n",
    "        plm_name=PLM_NAME,\n",
    "        embeddings_path=EMBEDDINGS_PATH,\n",
    "        frozen_plm=FROZEN_PLM,\n",
    "        gradient_checkpointing=not FROZEN_PLM,\n",
    "        fc_hidden=512,\n",
    "        fc_dropout=0.1,\n",
    "        head_strategy=HEAD_STRATEGY,\n",
    "        head_hidden=256,\n",
    "        head_dropout=0.1,\n",
    "        max_seq_length=512,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        gradient_clip=1.0,\n",
    "        loss_type=LOSS_TYPE,\n",
    "        focal_gamma=1.0,\n",
    "        q8_loss_weight=1.0,\n",
    "        q3_loss_weight=0.5,\n",
    "        checkpoint_dir=OUTPUT_DIR,\n",
    "        use_tracking=False,  # Disable for Kaggle\n",
    "    )\n",
    "    ModelClass = Tier1Baseline\n",
    "\n",
    "elif TIER == 2:\n",
    "    config = Tier2Config(\n",
    "        plm_name=PLM_NAME,\n",
    "        embeddings_path=EMBEDDINGS_PATH,\n",
    "        frozen_plm=FROZEN_PLM,\n",
    "        gradient_checkpointing=not FROZEN_PLM,\n",
    "        cnn_type=CNN_TYPE,\n",
    "        kernel_sizes=[3, 5, 7, 11],\n",
    "        cnn_out_channels=64,\n",
    "        cnn_num_layers=4,\n",
    "        cnn_dilations=[1, 2, 4, 8],\n",
    "        cnn_residual=True,\n",
    "        cnn_dropout=0.0,\n",
    "        head_strategy=HEAD_STRATEGY,\n",
    "        head_hidden=256,\n",
    "        head_dropout=0.1,\n",
    "        max_seq_length=512,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        gradient_clip=1.0,\n",
    "        loss_type=LOSS_TYPE,\n",
    "        focal_gamma=1.0,\n",
    "        q8_loss_weight=1.0,\n",
    "        q3_loss_weight=0.5,\n",
    "        checkpoint_dir=OUTPUT_DIR,\n",
    "        use_tracking=False,\n",
    "    )\n",
    "    ModelClass = Tier2CNN\n",
    "\n",
    "elif TIER == 3:\n",
    "    config = Tier3Config(\n",
    "        plm_name=PLM_NAME,\n",
    "        embeddings_path=EMBEDDINGS_PATH,\n",
    "        frozen_plm=FROZEN_PLM,\n",
    "        gradient_checkpointing=not FROZEN_PLM,\n",
    "        skip_cnn=not FROZEN_PLM,  # Skip CNN in FFT mode (PLM already captures features)\n",
    "        cnn_type=CNN_TYPE,\n",
    "        kernel_sizes=[3, 5, 7],\n",
    "        cnn_out_channels=64,\n",
    "        cnn_num_layers=4,\n",
    "        cnn_dilations=[1, 2, 4, 8],\n",
    "        cnn_residual=True,\n",
    "        cnn_dropout=0.0,\n",
    "        rnn_type=RNN_TYPE,\n",
    "        rnn_hidden=256,\n",
    "        rnn_layers=2,\n",
    "        rnn_dropout=0.3,\n",
    "        rnn_bidirectional=True,\n",
    "        head_strategy=HEAD_STRATEGY,\n",
    "        head_hidden=256,\n",
    "        head_dropout=0.1,\n",
    "        max_seq_length=512,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        gradient_clip=1.0,\n",
    "        loss_type=LOSS_TYPE,\n",
    "        focal_gamma=1.0,\n",
    "        q8_loss_weight=1.0,\n",
    "        q3_loss_weight=0.5,\n",
    "        checkpoint_dir=OUTPUT_DIR,\n",
    "        use_tracking=False,\n",
    "    )\n",
    "    ModelClass = Tier3CNNRNN\n",
    "\n",
    "print(f\"âœ“ Tier {TIER} config created\")\n",
    "print(f\"   Mode: {'Frozen' if FROZEN_PLM else 'FFT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441470f3",
   "metadata": {},
   "source": [
    "## 5. Load Data\n",
    "\n",
    "- **Frozen mode**: Loads pre-extracted embeddings from HDF5 (fast, requires embedding file)\n",
    "- **FFT mode**: Loads raw sequences (PLM tokenizes on-the-fly, no embedding file needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FROZEN_PLM:\n",
    "    # FROZEN MODE: Check embeddings exist\n",
    "    embeddings_path = Path(EMBEDDINGS_PATH)\n",
    "    if not embeddings_path.exists():\n",
    "        print(f\"âŒ Embeddings not found: {embeddings_path}\")\n",
    "        print(f\"\\n   You need to either:\")\n",
    "        print(f\"   1. Upload pre-extracted embeddings as a Kaggle dataset\")\n",
    "        print(f\"   2. Set FROZEN_PLM = False to use FFT mode (no embeddings needed)\")\n",
    "        print(f\"   3. Run the embedding extraction section at the bottom\")\n",
    "    else:\n",
    "        import h5py\n",
    "        with h5py.File(embeddings_path, 'r') as f:\n",
    "            train_count = len(f['train']) if 'train' in f else 0\n",
    "            test_count = len(f['test']) if 'test' in f else 0\n",
    "            plm_name = f.attrs.get('plm_name', 'unknown')\n",
    "            emb_dim = f.attrs.get('embedding_dim', 0)\n",
    "        \n",
    "        print(f\"âœ“ Embeddings found: {embeddings_path}\")\n",
    "        print(f\"   PLM: {plm_name}, Dim: {emb_dim}\")\n",
    "        print(f\"   Train: {train_count}, Test: {test_count}\")\n",
    "else:\n",
    "    # FFT MODE: No embeddings needed\n",
    "    print(\"ğŸ”¥ FFT Mode: PLM will be trained end-to-end\")\n",
    "    print(f\"   PLM: {PLM_NAME}\")\n",
    "    print(f\"   No pre-extracted embeddings required!\")\n",
    "    print(f\"   âš ï¸  Requires ~16GB+ GPU memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7add70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "\n",
    "if FROZEN_PLM:\n",
    "    # Frozen mode: use pre-computed embeddings\n",
    "    full_dataset = HDF5EmbeddingDataset(\n",
    "        csv_path=TRAIN_CSV,\n",
    "        h5_path=EMBEDDINGS_PATH,\n",
    "        dataset_name='train',\n",
    "        max_length=config.max_seq_length,\n",
    "        exclude_ids=LEAKAGE_TRAIN_IDS,\n",
    "    )\n",
    "    current_collate_fn = collate_fn\n",
    "else:\n",
    "    # FFT mode: load raw sequences\n",
    "    full_dataset = SequenceDataset(\n",
    "        csv_path=TRAIN_CSV,\n",
    "        max_length=config.max_seq_length,\n",
    "        exclude_ids=LEAKAGE_TRAIN_IDS,\n",
    "    )\n",
    "    current_collate_fn = collate_fn_sequences\n",
    "\n",
    "# Train/Val split\n",
    "val_size = int(len(full_dataset) * 0.1)\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=current_collate_fn,\n",
    "    num_workers=2 if FROZEN_PLM else 0,  # No multiprocessing for FFT\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=current_collate_fn,\n",
    "    num_workers=2 if FROZEN_PLM else 0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Split:\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples ({len(train_loader)} batches)\")\n",
    "print(f\"   Val:   {len(val_dataset):,} samples ({len(val_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d632f",
   "metadata": {},
   "source": [
    "## 6. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8538e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model based on tier\n",
    "if TIER == 1:\n",
    "    model = ModelClass(\n",
    "        embedding_dim=embedding_dim,\n",
    "        fc_hidden=config.fc_hidden,\n",
    "        fc_dropout=config.fc_dropout,\n",
    "        head_strategy=config.head_strategy,\n",
    "        head_hidden=config.head_hidden,\n",
    "        head_dropout=config.head_dropout,\n",
    "        frozen_plm=FROZEN_PLM,\n",
    "        plm_name=PLM_NAME if not FROZEN_PLM else None,\n",
    "        gradient_checkpointing=config.gradient_checkpointing if not FROZEN_PLM else False,\n",
    "    )\n",
    "    \n",
    "elif TIER == 2:\n",
    "    model = ModelClass(\n",
    "        embedding_dim=embedding_dim,\n",
    "        cnn_type=config.cnn_type,\n",
    "        kernel_sizes=config.kernel_sizes,\n",
    "        cnn_out_channels=config.cnn_out_channels,\n",
    "        cnn_num_layers=config.cnn_num_layers,\n",
    "        cnn_dilations=config.cnn_dilations,\n",
    "        cnn_activation='relu',\n",
    "        cnn_dropout=config.cnn_dropout,\n",
    "        cnn_residual=config.cnn_residual,\n",
    "        head_strategy=config.head_strategy,\n",
    "        head_hidden=config.head_hidden,\n",
    "        head_dropout=config.head_dropout,\n",
    "        frozen_plm=FROZEN_PLM,\n",
    "        plm_name=PLM_NAME if not FROZEN_PLM else None,\n",
    "        gradient_checkpointing=config.gradient_checkpointing if not FROZEN_PLM else False,\n",
    "    )\n",
    "    \n",
    "elif TIER == 3:\n",
    "    model = ModelClass(\n",
    "        embedding_dim=embedding_dim,\n",
    "        skip_cnn=config.skip_cnn,\n",
    "        cnn_type=config.cnn_type,\n",
    "        kernel_sizes=config.kernel_sizes,\n",
    "        cnn_out_channels=config.cnn_out_channels,\n",
    "        cnn_num_layers=config.cnn_num_layers,\n",
    "        cnn_dilations=config.cnn_dilations,\n",
    "        cnn_dropout=config.cnn_dropout,\n",
    "        cnn_residual=config.cnn_residual,\n",
    "        rnn_type=config.rnn_type,\n",
    "        rnn_hidden=config.rnn_hidden,\n",
    "        rnn_layers=config.rnn_layers,\n",
    "        rnn_dropout=config.rnn_dropout,\n",
    "        rnn_bidirectional=config.rnn_bidirectional,\n",
    "        head_strategy=config.head_strategy,\n",
    "        head_hidden=config.head_hidden,\n",
    "        head_dropout=config.head_dropout,\n",
    "        frozen_plm=FROZEN_PLM,\n",
    "        plm_name=PLM_NAME if not FROZEN_PLM else None,\n",
    "        gradient_checkpointing=config.gradient_checkpointing if not FROZEN_PLM else False,\n",
    "    )\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(f\"\\nğŸ—ï¸  Model: Tier {TIER}\")\n",
    "print(f\"ğŸ”§ Mode: {'Frozen PLM' if FROZEN_PLM else 'ğŸ”¥ FFT'}\")\n",
    "\n",
    "if FROZEN_PLM:\n",
    "    print(f\"ğŸ“ˆ Total Parameters: {model.count_parameters():,}\")\n",
    "else:\n",
    "    total_params = model.count_parameters()\n",
    "    head_params = model.count_head_parameters()\n",
    "    plm_params = total_params - head_params\n",
    "    print(f\"ğŸ“ˆ Parameter Breakdown:\")\n",
    "    print(f\"   PLM Backbone: {plm_params:,} (trainable)\")\n",
    "    print(f\"   Head Layers:  {head_params:,}\")\n",
    "    print(f\"   Total:        {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "sample_batch = next(iter(train_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    lengths = sample_batch['lengths']\n",
    "    \n",
    "    if FROZEN_PLM:\n",
    "        features = sample_batch['features'].to(DEVICE)\n",
    "        if TIER == 3:\n",
    "            q8_out, q3_out = model(features, lengths=lengths)\n",
    "        else:\n",
    "            q8_out, q3_out = model(features)\n",
    "        print(f\"âœ“ Forward pass: {features.shape} â†’ Q8 {q8_out.shape}, Q3 {q3_out.shape}\")\n",
    "    else:\n",
    "        sequences = sample_batch['sequences']\n",
    "        if TIER == 3:\n",
    "            q8_out, q3_out = model(sequences=sequences, lengths=lengths)\n",
    "        else:\n",
    "            q8_out, q3_out = model(sequences=sequences)\n",
    "        print(f\"âœ“ Forward pass: {len(sequences)} sequences â†’ Q8 {q8_out.shape}, Q3 {q3_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47329c40",
   "metadata": {},
   "source": [
    "## 7. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83487ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = get_multitask_loss(\n",
    "    loss_type=config.loss_type,\n",
    "    q8_weight=config.q8_loss_weight,\n",
    "    q3_weight=config.q3_loss_weight,\n",
    "    gamma=config.focal_gamma,\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = create_optimizer(\n",
    "    model,\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    optimizer_type='adamw',\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = create_scheduler(\n",
    "    optimizer,\n",
    "    scheduler_type='cosine',\n",
    "    num_epochs=config.max_epochs,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‰ Loss: {config.loss_type}\")\n",
    "print(f\"âš¡ Optimizer: AdamW (lr={config.learning_rate})\")\n",
    "print(f\"ğŸ“… Scheduler: CosineAnnealing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b156c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=config.checkpoint_dir,\n",
    "    gradient_clip=config.gradient_clip,\n",
    "    log_every=100,\n",
    "    use_amp=torch.cuda.is_available(),\n",
    "    use_tracking=False,  # Disabled for Kaggle\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(f\"   Checkpoint dir: {config.checkpoint_dir}\")\n",
    "print(f\"   Mixed Precision: {trainer.use_amp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37eb6f9",
   "metadata": {},
   "source": [
    "## 8. Train Model ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128be2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"â•\" * 60)\n",
    "print(\"ğŸš€ STARTING TRAINING\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "history = trainer.train(\n",
    "    num_epochs=config.max_epochs,\n",
    "    patience=config.patience,\n",
    "    save_every=5,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 60)\n",
    "print(\"âœ… TRAINING COMPLETE\")\n",
    "print(\"â•\" * 60)\n",
    "print(f\"\\nğŸ“ˆ Best Results:\")\n",
    "print(f\"   Harmonic F1: {trainer.best_harmonic_f1:.4f}\")\n",
    "print(f\"   Q8 F1:       {trainer.best_q8_f1:.4f}\")\n",
    "print(f\"   Q8 Accuracy: {trainer.best_q8_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ff25c",
   "metadata": {},
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_SUBMISSION:\n",
    "    print(\"\\n\" + \"â•\" * 60)\n",
    "    print(\"ğŸ“ GENERATING SUBMISSION\")\n",
    "    print(\"â•\" * 60)\n",
    "    \n",
    "    # Load best model\n",
    "    best_checkpoint = torch.load(\n",
    "        Path(OUTPUT_DIR) / 'best_model.pt',\n",
    "        map_location=DEVICE\n",
    "    )\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"âœ“ Best model loaded (epoch {best_checkpoint.get('epoch', 'unknown')})\")\n",
    "    \n",
    "    # Load test data\n",
    "    if FROZEN_PLM:\n",
    "        test_dataset = HDF5EmbeddingDataset(\n",
    "            csv_path=TEST_CSV,\n",
    "            h5_path=EMBEDDINGS_PATH,\n",
    "            dataset_name='test',\n",
    "            max_length=config.max_seq_length,\n",
    "            is_test=True,\n",
    "        )\n",
    "        test_collate = collate_fn\n",
    "    else:\n",
    "        test_dataset = SequenceDataset(\n",
    "            csv_path=TEST_CSV,\n",
    "            max_length=config.max_seq_length,\n",
    "            is_test=True,\n",
    "        )\n",
    "        test_collate = collate_fn_sequences\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=test_collate,\n",
    "        num_workers=2 if FROZEN_PLM else 0,\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Test set loaded: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            lengths = batch['lengths']\n",
    "            ids = batch['ids']\n",
    "            \n",
    "            # Forward pass\n",
    "            if FROZEN_PLM:\n",
    "                features = batch['features'].to(DEVICE)\n",
    "                if TIER == 3:\n",
    "                    q8_logits, _ = model(features, lengths=lengths, return_q3=False)\n",
    "                else:\n",
    "                    q8_logits, _ = model(features, return_q3=False)\n",
    "            else:\n",
    "                sequences = batch['sequences']\n",
    "                if TIER == 3:\n",
    "                    q8_logits, _ = model(sequences=sequences, lengths=lengths, return_q3=False)\n",
    "                else:\n",
    "                    q8_logits, _ = model(sequences=sequences, return_q3=False)\n",
    "            \n",
    "            q8_preds = q8_logits.argmax(dim=-1)  # (batch, seq_len)\n",
    "            \n",
    "            # Convert to strings\n",
    "            for i, (sample_id, length) in enumerate(zip(ids, lengths)):\n",
    "                pred_indices = q8_preds[i, :length].cpu().numpy()\n",
    "                pred_str = ''.join([IDX_TO_SST8[idx] for idx in pred_indices])\n",
    "                all_ids.append(sample_id)\n",
    "                all_preds.append(pred_str)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': all_ids,\n",
    "        'sst8': all_preds,\n",
    "    })\n",
    "    \n",
    "    # Save submission\n",
    "    submission_path = Path(OUTPUT_DIR) / 'submission.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Submission saved: {submission_path}\")\n",
    "    print(f\"   Total predictions: {len(submission_df)}\")\n",
    "    print(f\"\\nğŸ“‹ Preview:\")\n",
    "    print(submission_df.head(10))\n",
    "    \n",
    "    # For Kaggle, also save to /kaggle/working for easy download\n",
    "    # submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "else:\n",
    "    print(\"â„¹ï¸  Submission generation disabled. Set GENERATE_SUBMISSION = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f811fc",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71134b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"â•\" * 60)\n",
    "print(\"ğŸ‰ KAGGLE NOTEBOOK COMPLETE\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ—ï¸  Model Configuration:\")\n",
    "print(f\"   Tier: {TIER}\")\n",
    "print(f\"   Mode: {'Frozen PLM' if FROZEN_PLM else 'ğŸ”¥ FFT'}\")\n",
    "print(f\"   PLM: {PLM_NAME}\")\n",
    "if TIER >= 2:\n",
    "    print(f\"   CNN: {CNN_TYPE}\")\n",
    "if TIER >= 3:\n",
    "    print(f\"   RNN: {RNN_TYPE}\")\n",
    "print(f\"   Loss: {LOSS_TYPE}\")\n",
    "print(f\"   Head: {HEAD_STRATEGY}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Training Results:\")\n",
    "print(f\"   Best Harmonic F1: {trainer.best_harmonic_f1:.4f}\")\n",
    "print(f\"   Best Q8 F1:       {trainer.best_q8_f1:.4f}\")\n",
    "print(f\"   Best Q8 Accuracy: {trainer.best_q8_accuracy:.4f}\")\n",
    "\n",
    "if GENERATE_SUBMISSION:\n",
    "    print(f\"\\nğŸ“ Submission:\")\n",
    "    print(f\"   File: {submission_path}\")\n",
    "    print(f\"   Predictions: {len(submission_df)}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saved Files:\")\n",
    "print(f\"   {OUTPUT_DIR}/best_model.pt\")\n",
    "print(f\"   {OUTPUT_DIR}/submission.csv\")\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 60)\n",
    "print(\"ğŸš€ Ready to submit to Kaggle!\")\n",
    "print(\"â•\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac5600",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Appendix A: Extract Embeddings (For Frozen Mode)\n",
    "\n",
    "Run this section **only if** you want to use `FROZEN_PLM = True` but don't have embeddings yet.\n",
    "After extraction, save the `.h5` file as a Kaggle Dataset for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EMBEDDING EXTRACTION (For Frozen Mode)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "EXTRACT_EMBEDDINGS = False  # Set to True to extract\n",
    "\n",
    "if EXTRACT_EMBEDDINGS:\n",
    "    import h5py\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    \n",
    "    # PLM to extract\n",
    "    EXTRACT_PLM = 'protbert'  # or 'esm2_8m', 'esm2_35m', 'esm2_650m'\n",
    "    \n",
    "    # PLM registry\n",
    "    PLM_REGISTRY = {\n",
    "        'protbert': ('Rostlab/prot_bert_bfd', 1024),\n",
    "        'esm2_8m': ('facebook/esm2_t6_8M_UR50D', 320),\n",
    "        'esm2_35m': ('facebook/esm2_t12_35M_UR50D', 480),\n",
    "        'esm2_650m': ('facebook/esm2_t33_650M_UR50D', 1280),\n",
    "    }\n",
    "    \n",
    "    model_name, emb_dim = PLM_REGISTRY[EXTRACT_PLM]\n",
    "    \n",
    "    print(f\"Loading {EXTRACT_PLM}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    plm_model = AutoModel.from_pretrained(model_name).eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        plm_model = plm_model.cuda()\n",
    "    \n",
    "    def extract_batch(sequences, batch_size=8):\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "            batch_seqs = sequences[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize (add spaces for ProtBert)\n",
    "            if 'protbert' in EXTRACT_PLM:\n",
    "                batch_seqs = [' '.join(list(seq)) for seq in batch_seqs]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_seqs,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = plm_model(**inputs).last_hidden_state\n",
    "            \n",
    "            # Extract embeddings (remove special tokens)\n",
    "            for j, seq in enumerate(batch_seqs):\n",
    "                if 'protbert' in EXTRACT_PLM:\n",
    "                    seq_len = len(seq.split())\n",
    "                    emb = outputs[j, 1:seq_len+1, :].cpu().numpy()\n",
    "                else:\n",
    "                    seq_len = len(sequences[i+j])\n",
    "                    emb = outputs[j, 1:seq_len+1, :].cpu().numpy()\n",
    "                embeddings.append(emb)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    \n",
    "    print(f\"\\nExtracting train embeddings ({len(train_df)} samples)...\")\n",
    "    train_embs = extract_batch(train_df['sequence'].tolist())\n",
    "    \n",
    "    print(f\"\\nExtracting test embeddings ({len(test_df)} samples)...\")\n",
    "    test_embs = extract_batch(test_df['sequence'].tolist())\n",
    "    \n",
    "    # Save to HDF5\n",
    "    output_h5 = f'{OUTPUT_DIR}/{EXTRACT_PLM}.h5'\n",
    "    print(f\"\\nSaving to {output_h5}...\")\n",
    "    \n",
    "    with h5py.File(output_h5, 'w') as f:\n",
    "        f.attrs['plm_name'] = EXTRACT_PLM\n",
    "        f.attrs['embedding_dim'] = emb_dim\n",
    "        \n",
    "        train_grp = f.create_group('train')\n",
    "        for i, emb in enumerate(train_embs):\n",
    "            train_grp.create_dataset(str(train_df.iloc[i]['id']), data=emb)\n",
    "        \n",
    "        test_grp = f.create_group('test')\n",
    "        for i, emb in enumerate(test_embs):\n",
    "            test_grp.create_dataset(str(test_df.iloc[i]['id']), data=emb)\n",
    "    \n",
    "    print(f\"âœ… Embeddings saved to {output_h5}\")\n",
    "    print(f\"   Download this file and upload as a Kaggle Dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dfa1a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Appendix B: FFT Mode Tips\n",
    "\n",
    "When using `FROZEN_PLM = False` (Full Fine-Tuning):\n",
    "\n",
    "| Setting | Recommendation |\n",
    "|---------|---------------|\n",
    "| **GPU** | Use Kaggle's P100 (16GB) or better |\n",
    "| **Batch Size** | 4-8 (adjust based on memory) |\n",
    "| **Learning Rate** | 1e-5 to 5e-6 |\n",
    "| **Epochs** | 10-20 (PLM converges faster) |\n",
    "| **Gradient Checkpointing** | Enabled by default (saves memory) |\n",
    "\n",
    "### Memory Optimization\n",
    "If you run out of memory:\n",
    "1. Reduce `BATCH_SIZE` to 2-4\n",
    "2. Use smaller PLM (`esm2_8m` instead of `protbert`)\n",
    "3. Reduce `max_seq_length` to 256\n",
    "4. For Tier 3, set `skip_cnn=True` in config"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
